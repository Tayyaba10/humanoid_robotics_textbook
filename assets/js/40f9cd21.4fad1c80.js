"use strict";(globalThis.webpackChunkhumanoid_robotics_textbook=globalThis.webpackChunkhumanoid_robotics_textbook||[]).push([[346],{821:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>u,default:()=>g,frontMatter:()=>d,metadata:()=>r,toc:()=>m});const r=JSON.parse('{"id":"embodied-learning","title":"8. Embodied Learning: RL and IL","description":"How robots can learn from experience and demonstration.","source":"@site/docs/08-embodied-learning.mdx","sourceDirName":".","slug":"/embodied-learning","permalink":"/humanoid_robotics_textbook/docs/embodied-learning","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"id":"embodied-learning","title":"8. Embodied Learning: RL and IL","description":"How robots can learn from experience and demonstration.","sidebar_position":8},"sidebar":"tutorialSidebar","previous":{"title":"7. Planning & Decision-Making","permalink":"/humanoid_robotics_textbook/docs/planning-decision-making"},"next":{"title":"9. Foundation Models in Robotics","permalink":"/humanoid_robotics_textbook/docs/foundation-models"}}');var a=t(4848),i=t(8453),o=t(418),s=t(3457),l=t(1470),c=t(9365);const d={id:"embodied-learning",title:"8. Embodied Learning: RL and IL",description:"How robots can learn from experience and demonstration.",sidebar_position:8},u=void 0,h={},m=[{value:"When Hand-Coding Isn&#39;t Enough",id:"when-hand-coding-isnt-enough",level:2},{value:"Reinforcement Learning (RL): Learning from Trial and Error",id:"reinforcement-learning-rl-learning-from-trial-and-error",level:3},{value:"Imitation Learning (IL): Learning from an Expert",id:"imitation-learning-il-learning-from-an-expert",level:3},{value:"Behavioral Cloning (BC)",id:"behavioral-cloning-bc",level:4},{value:"Inverse Reinforcement Learning (IRL)",id:"inverse-reinforcement-learning-irl",level:4},{value:"RL vs. IL: A Summary",id:"rl-vs-il-a-summary",level:3},{value:"Code Example: Behavioral Cloning Data",id:"code-example-behavioral-cloning-data",level:3}];function p(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"when-hand-coding-isnt-enough",children:"When Hand-Coding Isn't Enough"}),"\n",(0,a.jsx)(n.p,{children:"For many tasks, like moving an arm to a specific point, we can design a controller based on well-understood physics and geometry. But what about tasks that require intuition, dexterity, or reacting to complex, dynamic contact? How would you program a robot to fold a towel or gracefully recover from a stumble?"}),"\n",(0,a.jsxs)(n.p,{children:["This is where ",(0,a.jsx)(n.strong,{children:"embodied learning"})," comes in. Instead of explicitly programming a behavior, we create a framework for the robot to learn it, either by practicing itself or by observing an expert. This chapter covers the two dominant paradigms: Reinforcement Learning (RL) and Imitation Learning (IL)."]}),"\n",(0,a.jsx)(n.h3,{id:"reinforcement-learning-rl-learning-from-trial-and-error",children:"Reinforcement Learning (RL): Learning from Trial and Error"}),"\n",(0,a.jsx)(n.p,{children:"Reinforcement Learning is analogous to how you might train a pet. You give it a command, and if it performs the correct action, you give it a treat. If it does the wrong thing, it gets no treat. Over time, the pet learns which actions lead to a reward."}),"\n",(0,a.jsxs)(n.p,{children:['In RL, the "pet" is the ',(0,a.jsx)(n.strong,{children:"agent"})," (the robot's control policy). It exists in an ",(0,a.jsx)(n.strong,{children:"environment"})," and can take ",(0,a.jsx)(n.strong,{children:"actions"}),". After each action, the environment gives the agent a ",(0,a.jsx)(n.strong,{children:"reward"})," (a numerical score) and a new ",(0,a.jsx)(n.strong,{children:"state"})," (the new sensor readings)."]}),"\n",(0,a.jsxs)(n.p,{children:["The agent's goal is to learn a ",(0,a.jsx)(n.strong,{children:"policy"}),"\u2014a strategy that maps states to actions\u2014that maximizes its total cumulative reward over time."]}),"\n",(0,a.jsx)(o.A,{chart:'\ngraph TD\n  subgraph "Agent (Robot\'s Brain)"\n      A(Policy);\n  end\n  subgraph "Environment (The World)"\n      B(State);\n      C(Reward);\n  end\n  \n  A --"Action"--\x3e B;\n  B --\x3e A;\n  C -.-> A;\n\n  linkStyle 2 stroke-dasharray: 5 5;\n'}),"\n",(0,a.jsxs)(n.p,{children:["The core challenge of RL for robotics is the sheer amount of practice required. A learning algorithm might need millions of trials to master a task. This is usually impossible to do on a real robot due to time, cost, and safety concerns. As a result, most RL for robotics is done in ",(0,a.jsx)(n.strong,{children:"simulation"}),', which introduces its own challenge of transferring the learned skill to the real world (the "sim-to-real" problem, discussed in Chapter 10).']}),"\n",(0,a.jsx)(n.h3,{id:"imitation-learning-il-learning-from-an-expert",children:"Imitation Learning (IL): Learning from an Expert"}),"\n",(0,a.jsx)(n.p,{children:"If RL is learning by trial and error, Imitation Learning is learning by watching a master. Instead of a reward signal, the learning algorithm is given a dataset of expert demonstrations. This is often a more direct and efficient way to teach a robot a specific skill."}),"\n",(0,a.jsx)(n.h4,{id:"behavioral-cloning-bc",children:"Behavioral Cloning (BC)"}),"\n",(0,a.jsx)(n.p,{children:"This is the simplest and most direct form of IL. It's a standard supervised learning problem."}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data:"})," A large dataset of ",(0,a.jsx)(n.code,{children:"(state, expert_action)"})," pairs. For example, a human teleoperates a robot arm, and you record the camera images (state) and the corresponding joystick commands (expert_action) at each step."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Goal:"})," Train a model (typically a neural network) that takes a state as input and outputs the action the expert would have taken."]}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["BC is powerful and simple, but it has a key weakness: ",(0,a.jsx)(n.strong,{children:"distributional shift"}),". If the robot makes a small mistake and enters a state that wasn't in the expert's dataset, it has no idea what to do and can drift further and further off course."]}),"\n",(0,a.jsx)(n.h4,{id:"inverse-reinforcement-learning-irl",children:"Inverse Reinforcement Learning (IRL)"}),"\n",(0,a.jsxs)(n.p,{children:["IRL is a more advanced and powerful form of IL. Instead of just copying the expert's actions, it tries to understand the expert's ",(0,a.jsx)(n.em,{children:"intent"}),"."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Goal:"})," The algorithm observes the expert's state-action pairs and tries to reverse-engineer the reward function the expert was implicitly optimizing."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Process:"})," Once the reward function is learned, the robot can use standard Reinforcement Learning to train a policy."]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"This approach is more robust than BC because if the robot finds itself in a novel state, it can still use the learned reward function to figure out the best action to take to get back on track."}),"\n",(0,a.jsx)(o.A,{chart:"\ngraph TD\n  subgraph Behavioral Cloning\n      A(State) --\x3e B(Policy);\n      B --\x3e C(Action);\n  end\n\n  subgraph Inverse Reinforcement Learning\n      D(State, Action) --\x3e E{Reward Function};\n      E --\x3e F(Reinforcement Learning);\n      F --\x3e G(Policy);\n  end\n"}),"\n",(0,a.jsx)(n.h3,{id:"rl-vs-il-a-summary",children:"RL vs. IL: A Summary"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reinforcement Learning:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pros:"})," Can discover novel, superhuman behaviors."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cons:"})," Requires a carefully designed reward function and massive amounts of exploration."]}),"\n"]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Imitation Learning:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pros:"})," Simpler to start with (just need demonstrations), bypasses the need for reward design."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Cons:"})," The learned skill is fundamentally limited by the expert's ability and the diversity of the demonstrations."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"In modern robotics, these techniques are often blended. A robot might be pre-trained using IL to get a good starting policy, which is then fine-tuned with RL to make it more robust and optimal."}),"\n",(0,a.jsx)(n.h3,{id:"code-example-behavioral-cloning-data",children:"Code Example: Behavioral Cloning Data"}),"\n",(0,a.jsxs)(n.p,{children:["We can't train a full model here, but we can illustrate the fundamental data structure for a Behavioral Cloning problem. The goal is to create a dataset of ",(0,a.jsx)(n.code,{children:"(state, action)"})," pairs that a supervised learning model could use."]}),"\n",(0,a.jsx)(l.A,{children:(0,a.jsx)(c.A,{value:"python",label:"Python",children:(0,a.jsx)(s.A,{language:"python",children:(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\n\nclass ExpertDemonstrationRecorder:\n    """A simple class to simulate the recording of expert demonstrations."""\n    \n    def __init__(self):\n        # Our dataset is a list of (state, action) tuples.\n        self.demonstrations = []\n\n    def record_step(self, camera_image, joint_angles, expert_joystick_input):\n        """\n        In a real system, this would be called in a loop as the expert\n        controls the robot.\n        """\n        # 1. Flatten and concatenate sensor data to form the \'state\' vector.\n        state = np.concatenate([\n            camera_image.flatten(),\n            joint_angles.flatten()\n        ])\n        \n        # 2. The \'action\' is the expert\'s command.\n        action = expert_joystick_input\n        \n        # 3. Add the (state, action) pair to our dataset.\n        self.demonstrations.append((state, action))\n        print(f"Recorded step. State shape: {state.shape}, Action: {action}")\n\n    def get_dataset(self):\n        """Returns the dataset for training."""\n        return self.demonstrations\n\n# --- Simulation of collecting two steps ---\nrecorder = ExpertDemonstrationRecorder()\n\n# Step 1: Expert sees the initial scene and moves the joystick.\nsim_camera_img_1 = np.random.rand(120, 160) # Simulated low-res camera image\nsim_joint_angles_1 = np.array([0.5, 0.2, -0.3]) # Robot\'s joint angles\nsim_joystick_1 = np.array([0.9, -0.1]) # Expert commands: move forward-right\nrecorder.record_step(sim_camera_img_1, sim_joint_angles_1, sim_joystick_1)\n\n# Step 2: Expert sees the new scene and adjusts the robot.\nsim_camera_img_2 = np.random.rand(120, 160)\nsim_joint_angles_2 = np.array([0.6, 0.15, -0.35])\nsim_joystick_2 = np.array([0.8, -0.2]) # Expert continues forward-right\nrecorder.record_step(sim_camera_img_2, sim_joint_angles_2, sim_joystick_2)\n\n# Now, `recorder.demonstrations` holds the data needed to train a\n# behavioral cloning model. The model\'s job would be to learn a function `f`\n# where `f(state) -> expert_action`.\n'})})})})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{})})]})}function g(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(p,{...e})}):p(e)}},1470:(e,n,t)=>{t.d(n,{A:()=>_});var r=t(6540),a=t(4164),i=t(7559),o=t(3104),s=t(6347),l=t(205),c=t(7485),d=t(1682),u=t(679);function h(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),a=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(a),(0,r.useCallback)(e=>{if(!a)return;const n=new URLSearchParams(t.location.search);n.set(a,e),t.replace({...t.location,search:n.toString()})},[a,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:a}=e,i=m(e),[o,s]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:i})),[c,d]=g({queryString:t,groupId:a}),[h,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,a]=(0,u.Dv)(n);return[t,(0,r.useCallback)(e=>{n&&a.set(e)},[n,a])]}({groupId:a}),b=(()=>{const e=c??h;return p({value:e,tabValues:i})?e:null})();(0,l.A)(()=>{b&&s(b)},[b]);return{selectedValue:o,selectValue:(0,r.useCallback)(e=>{if(!p({value:e,tabValues:i}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),f(e)},[d,f,i]),tabValues:i}}var b=t(2303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=t(4848);function j({className:e,block:n,selectedValue:t,selectValue:r,tabValues:i}){const s=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const n=e.currentTarget,a=s.indexOf(n),o=i[a].value;o!==t&&(l(n),r(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,a.A)("tabs",{"tabs--block":n},e),children:i.map(({value:e,label:n,attributes:r})=>(0,v.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:c,...r,className:(0,a.A)("tabs__item",x.tabItem,r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function y({lazy:e,children:n,selectedValue:t}){const i=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=i.find(e=>e.props.value===t);return e?(0,r.cloneElement)(e,{className:(0,a.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:i.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function w(e){const n=f(e);return(0,v.jsxs)("div",{className:(0,a.A)(i.G.tabs.container,"tabs-container",x.tabList),children:[(0,v.jsx)(j,{...n,...e}),(0,v.jsx)(y,{...n,...e})]})}function _(e){const n=(0,b.A)();return(0,v.jsx)(w,{...e,children:h(e.children)},String(n))}},9365:(e,n,t)=>{t.d(n,{A:()=>o});t(6540);var r=t(4164);const a={tabItem:"tabItem_Ymn6"};var i=t(4848);function o({children:e,hidden:n,className:t}){return(0,i.jsx)("div",{role:"tabpanel",className:(0,r.A)(a.tabItem,t),hidden:n,children:e})}}}]);