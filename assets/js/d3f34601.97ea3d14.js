"use strict";(globalThis.webpackChunkhumanoid_robotics_textbook=globalThis.webpackChunkhumanoid_robotics_textbook||[]).push([[794],{1470:(e,n,t)=>{t.d(n,{A:()=>k});var a=t(6540),o=t(4164),s=t(7559),r=t(3104),i=t(6347),l=t(205),c=t(7485),d=t(1682),u=t(679);function h(e){return a.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,a.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,a.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:t,default:a}})=>({value:e,label:n,attributes:t,default:a}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const t=(0,i.W6)(),o=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(o),(0,a.useCallback)(e=>{if(!o)return;const n=new URLSearchParams(t.location.search);n.set(o,e),t.replace({...t.location,search:n.toString()})},[o,t])]}function b(e){const{defaultValue:n,queryString:t=!1,groupId:o}=e,s=m(e),[r,i]=(0,a.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:s})),[c,d]=g({queryString:t,groupId:o}),[h,b]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,o]=(0,u.Dv)(n);return[t,(0,a.useCallback)(e=>{n&&o.set(e)},[n,o])]}({groupId:o}),f=(()=>{const e=c??h;return p({value:e,tabValues:s})?e:null})();(0,l.A)(()=>{f&&i(f)},[f]);return{selectedValue:r,selectValue:(0,a.useCallback)(e=>{if(!p({value:e,tabValues:s}))throw new Error(`Can't select invalid tab value=${e}`);i(e),d(e),b(e)},[d,b,s]),tabValues:s}}var f=t(2303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=t(4848);function j({className:e,block:n,selectedValue:t,selectValue:a,tabValues:s}){const i=[],{blockElementScrollPositionUntilNextRender:l}=(0,r.a_)(),c=e=>{const n=e.currentTarget,o=i.indexOf(n),r=s[o].value;r!==t&&(l(n),a(r))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=i.indexOf(e.currentTarget)+1;n=i[t]??i[0];break}case"ArrowLeft":{const t=i.indexOf(e.currentTarget)-1;n=i[t]??i[i.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,o.A)("tabs",{"tabs--block":n},e),children:s.map(({value:e,label:n,attributes:a})=>(0,v.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{i.push(e)},onKeyDown:d,onClick:c,...a,className:(0,o.A)("tabs__item",x.tabItem,a?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function w({lazy:e,children:n,selectedValue:t}){const s=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=s.find(e=>e.props.value===t);return e?(0,a.cloneElement)(e,{className:(0,o.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:s.map((e,n)=>(0,a.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function L(e){const n=b(e);return(0,v.jsxs)("div",{className:(0,o.A)(s.G.tabs.container,"tabs-container",x.tabList),children:[(0,v.jsx)(j,{...n,...e}),(0,v.jsx)(w,{...n,...e})]})}function k(e){const n=(0,f.A)();return(0,v.jsx)(L,{...e,children:h(e.children)},String(n))}},9084:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>h,contentTitle:()=>u,default:()=>g,frontMatter:()=>d,metadata:()=>a,toc:()=>m});const a=JSON.parse('{"id":"foundation-models","title":"9. Foundation Models in Robotics","description":"The application of large-scale, pre-trained models to robotics tasks.","source":"@site/docs/09-foundation-models.mdx","sourceDirName":".","slug":"/foundation-models","permalink":"/humanoid_robotics_textbook/docs/foundation-models","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"id":"foundation-models","title":"9. Foundation Models in Robotics","description":"The application of large-scale, pre-trained models to robotics tasks.","sidebar_position":9},"sidebar":"tutorialSidebar","previous":{"title":"8. Embodied Learning: RL and IL","permalink":"/humanoid_robotics_textbook/docs/embodied-learning"},"next":{"title":"10. Simulation & Sim-to-Real","permalink":"/humanoid_robotics_textbook/docs/simulation-sim-to-real"}}');var o=t(4848),s=t(8453),r=t(418),i=t(3457),l=t(1470),c=t(9365);const d={id:"foundation-models",title:"9. Foundation Models in Robotics",description:"The application of large-scale, pre-trained models to robotics tasks.",sidebar_position:9},u=void 0,h={},m=[{value:"Giving Robots &quot;Common Sense&quot;",id:"giving-robots-common-sense",level:2},{value:"Vision-Language Models (VLMs): Understanding &quot;What&quot;",id:"vision-language-models-vlms-understanding-what",level:3},{value:"Large Language Models (LLMs): Understanding &quot;How&quot;",id:"large-language-models-llms-understanding-how",level:3},{value:"The New Architecture: VLM + LLM",id:"the-new-architecture-vlm--llm",level:3},{value:"Code Example: LLM for Task Decomposition",id:"code-example-llm-for-task-decomposition",level:3}];function p(e){const n={code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h2,{id:"giving-robots-common-sense",children:'Giving Robots "Common Sense"'}),"\n",(0,o.jsx)(n.p,{children:'One of the greatest challenges in robotics is bridging the gap between specific, low-level actions and abstract, high-level human goals. How does a robot translate the command "clean up the desk" into a series of concrete motions? This requires a degree of "common sense" reasoning that has historically been missing from robotic systems.'}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Foundation Models"})," are a new class of AI models that are changing the game. These are massive neural networks trained on internet-scale datasets (e.g., billions of images and trillions of words). This vast training allows them to learn rich, general-purpose representations of the world that can be adapted to many different tasks."]}),"\n",(0,o.jsx)(n.p,{children:'In robotics, these models are not used for low-level control (like balancing). Instead, they are used at the very top of the planning stack to provide the semantic reasoning and "common sense" that was previously lacking.'}),"\n",(0,o.jsx)(n.h3,{id:"vision-language-models-vlms-understanding-what",children:'Vision-Language Models (VLMs): Understanding "What"'}),"\n",(0,o.jsxs)(n.p,{children:["A ",(0,o.jsx)(n.strong,{children:"Vision-Language Model"})," (VLM), such as OpenAI's CLIP or Google's PaLI, is trained on a massive dataset of image-text pairs. As a result, it learns to connect visual information with natural language concepts."]}),"\n",(0,o.jsxs)(n.p,{children:["For a robot, this is a superpower. A traditional vision system can only detect the object categories it was explicitly trained on (e.g., 'cup', 'bottle'). A VLM allows for ",(0,o.jsx)(n.strong,{children:"open-vocabulary scene understanding"}),"."]}),"\n",(0,o.jsx)(n.p,{children:"You can give the robot a command like:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:['"Pick up the ',(0,o.jsx)(n.em,{children:"blue mug"}),'."']}),"\n",(0,o.jsxs)(n.li,{children:['"Find the ',(0,o.jsx)(n.em,{children:"can of sparkling water next to the microwave"}),'."']}),"\n",(0,o.jsxs)(n.li,{children:['"Hand me the ',(0,o.jsx)(n.em,{children:"largest red block"}),'."']}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"The VLM can understand these free-form descriptions and locate the corresponding objects in its camera feed, even if it has never seen that specific object before."}),"\n",(0,o.jsx)(r.A,{chart:'\ngraph TD\n  A[Camera Image] --\x3e B{VLM};\n  C[Text Prompt: "blue mug"] --\x3e B;\n  B --\x3e D[Location of Blue Mug];\n'}),"\n",(0,o.jsx)(n.h3,{id:"large-language-models-llms-understanding-how",children:'Large Language Models (LLMs): Understanding "How"'}),"\n",(0,o.jsxs)(n.p,{children:["A ",(0,o.jsx)(n.strong,{children:"Large Language Model"}),' (LLM), such as those in the GPT family, is an expert in sequence and reasoning, trained on the vast corpus of human text. In robotics, LLMs are used as high-level task planners. They act as the "common sense brain" that decomposes abstract goals into concrete steps.']}),"\n",(0,o.jsxs)(n.p,{children:["If a human gives the command, ",(0,o.jsx)(n.strong,{children:'"I\'m thirsty,"'})," an LLM can reason about the world and infer a plausible plan:"]}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Search for a bottle."}),"\n",(0,o.jsx)(n.li,{children:"If bottle is found, pick it up."}),"\n",(0,o.jsx)(n.li,{children:"Bring the bottle to the human."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This list of sub-tasks is then passed to the robot's mid-level task executor, such as a Behavior Tree, which knows how to perform each of those individual actions."}),"\n",(0,o.jsx)(r.A,{chart:'\ngraph TD\n  A[Human Command: "Get me a drink"] --\x3e B{LLM Task Planner};\n  B --\x3e C[Plan: [1. Find Bottle, 2. Pick Up, 3. Bring to Human]];\n  C --\x3e D(Behavior Tree Executor);\n'}),"\n",(0,o.jsx)(n.h3,{id:"the-new-architecture-vlm--llm",children:"The New Architecture: VLM + LLM"}),"\n",(0,o.jsx)(n.p,{children:"The most powerful approach combines these two models."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["The robot looks at a scene. The ",(0,o.jsx)(n.strong,{children:"VLM"})," processes the camera image and outputs a textual description: ",(0,o.jsx)(n.code,{children:'"There is a red can on the table and a blue bottle on the counter."'})]}),"\n",(0,o.jsxs)(n.li,{children:["This text, along with a human's command, is fed into the ",(0,o.jsx)(n.strong,{children:"LLM"}),". For example: ",(0,o.jsx)(n.code,{children:'World State: "A red can is on the table..." Human Command: "Get me the red can."'})]}),"\n",(0,o.jsxs)(n.li,{children:["The ",(0,o.jsx)(n.strong,{children:"LLM"})," acts as the reasoning engine. It combines the world state and the command to generate a high-level plan: ",(0,o.jsx)(n.code,{children:"[go_to_table, pick_up_red_can, bring_to_human]"}),"."]}),"\n",(0,o.jsx)(n.li,{children:"This plan is then executed by the robot's lower-level control systems."}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"This architecture allows for unprecedented flexibility and intelligence, enabling robots to respond to novel commands and reason about complex, unstructured environments."}),"\n",(0,o.jsx)(n.h3,{id:"code-example-llm-for-task-decomposition",children:"Code Example: LLM for Task Decomposition"}),"\n",(0,o.jsx)(n.p,{children:"We can't run a real LLM here, but we can simulate how a developer would interact with one. You send a text prompt and get back a structured plan."}),"\n",(0,o.jsx)(l.A,{children:(0,o.jsx)(c.A,{value:"python",label:"Python",children:(0,o.jsx)(i.A,{language:"python",children:'import json\n\ndef call_simulated_llm_planner(human_command, world_description):\n  """\n  Simulates making a call to an LLM to get a task plan.\n  In a real system, this would be an API call to a service like OpenAI or Google.\n  """\n  print(f"--- Calling Simulated LLM ---")\n  print(f"Human Command: \'{human_command}\'")\n  print(f"World State: \'{world_description}\'")\n  \n  # The magic of the LLM is its ability to generate this structured output\n  # from the unstructured text prompt.\n  if "tidy up" in human_command and "soda can" in world_description:\n      plan = [\n          {"action": "find", "object": "soda_can"},\n          {"action": "go_to", "object": "soda_can"},\n          {"action": "pick_up", "object": "soda_can"},\n          {"action": "find", "object": "trash_bin"},\n          {"action": "go_to", "object": "trash_bin"},\n          {"action": "drop_in", "object": "trash_bin"},\n      ]\n      print("LLM Response: Generated plan to throw away the can.")\n      return plan\n  else:\n      print("LLM Response: Could not determine a valid plan.")\n      return []\n\n# --- Simulation ---\n# 1. A VLM first scans the room and outputs a text description.\nworld_state = "A person is at the desk. A soda can is on the floor. A trash bin is in the corner."\n\n# 2. The human gives a high-level command.\ncommand = "Could you please tidy up the room?"\n\n# 3. We call the LLM to get a plan.\ntask_plan = call_simulated_llm_planner(command, world_state)\n\n# 4. The robot\'s task executor would now run this plan.\nprint("\n--- Robot Executing Plan ---")\nif task_plan:\n  for i, step in enumerate(task_plan):\n      print(f"Step {i+1}: Executing {step[\'action\']} on {step[\'object\']}")\nelse:\n  print("No plan to execute.")\n\n'})})})]})}function g(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}},9365:(e,n,t)=>{t.d(n,{A:()=>r});t(6540);var a=t(4164);const o={tabItem:"tabItem_Ymn6"};var s=t(4848);function r({children:e,hidden:n,className:t}){return(0,s.jsx)("div",{role:"tabpanel",className:(0,a.A)(o.tabItem,t),hidden:n,children:e})}}}]);