"use strict";(globalThis.webpackChunkhumanoid_robotics_textbook=globalThis.webpackChunkhumanoid_robotics_textbook||[]).push([[310],{1470:(e,n,t)=>{t.d(n,{A:()=>_});var r=t(6540),i=t(4164),a=t(7559),o=t(3104),s=t(6347),l=t(205),c=t(7485),d=t(1682),h=t(679);function u(e){return r.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,r.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function m(e){const{values:n,children:t}=e;return(0,r.useMemo)(()=>{const e=n??function(e){return u(e).map(({props:{value:e,label:n,attributes:t,default:r}})=>({value:e,label:n,attributes:t,default:r}))}(t);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,t])}function p({value:e,tabValues:n}){return n.some(n=>n.value===e)}function g({queryString:e=!1,groupId:n}){const t=(0,s.W6)(),i=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(i),(0,r.useCallback)(e=>{if(!i)return;const n=new URLSearchParams(t.location.search);n.set(i,e),t.replace({...t.location,search:n.toString()})},[i,t])]}function f(e){const{defaultValue:n,queryString:t=!1,groupId:i}=e,a=m(e),[o,s]=(0,r.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!p({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const t=n.find(e=>e.default)??n[0];if(!t)throw new Error("Unexpected error: 0 tabValues");return t.value}({defaultValue:n,tabValues:a})),[c,d]=g({queryString:t,groupId:i}),[u,f]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[t,i]=(0,h.Dv)(n);return[t,(0,r.useCallback)(e=>{n&&i.set(e)},[n,i])]}({groupId:i}),b=(()=>{const e=c??u;return p({value:e,tabValues:a})?e:null})();(0,l.A)(()=>{b&&s(b)},[b]);return{selectedValue:o,selectValue:(0,r.useCallback)(e=>{if(!p({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);s(e),d(e),f(e)},[d,f,a]),tabValues:a}}var b=t(2303);const x={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=t(4848);function j({className:e,block:n,selectedValue:t,selectValue:r,tabValues:a}){const s=[],{blockElementScrollPositionUntilNextRender:l}=(0,o.a_)(),c=e=>{const n=e.currentTarget,i=s.indexOf(n),o=a[i].value;o!==t&&(l(n),r(o))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const t=s.indexOf(e.currentTarget)+1;n=s[t]??s[0];break}case"ArrowLeft":{const t=s.indexOf(e.currentTarget)-1;n=s[t]??s[s.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,i.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:r})=>(0,v.jsx)("li",{role:"tab",tabIndex:t===e?0:-1,"aria-selected":t===e,ref:e=>{s.push(e)},onKeyDown:d,onClick:c,...r,className:(0,i.A)("tabs__item",x.tabItem,r?.className,{"tabs__item--active":t===e}),children:n??e},e))})}function w({lazy:e,children:n,selectedValue:t}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===t);return e?(0,r.cloneElement)(e,{className:(0,i.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,r.cloneElement)(e,{key:n,hidden:e.props.value!==t}))})}function y(e){const n=f(e);return(0,v.jsxs)("div",{className:(0,i.A)(a.G.tabs.container,"tabs-container",x.tabList),children:[(0,v.jsx)(j,{...n,...e}),(0,v.jsx)(w,{...n,...e})]})}function _(e){const n=(0,b.A)();return(0,v.jsx)(y,{...e,children:u(e.children)},String(n))}},9365:(e,n,t)=>{t.d(n,{A:()=>o});t(6540);var r=t(4164);const i={tabItem:"tabItem_Ymn6"};var a=t(4848);function o({children:e,hidden:n,className:t}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,r.A)(i.tabItem,t),hidden:n,children:e})}},9513:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>u,contentTitle:()=>h,default:()=>g,frontMatter:()=>d,metadata:()=>r,toc:()=>m});const r=JSON.parse('{"id":"perception-sensing","title":"3. Perception & Sensing","description":"Exploring the sensors and algorithms that allow a robot to perceive its environment.","source":"@site/docs/03-perception-sensing.mdx","sourceDirName":".","slug":"/perception-sensing","permalink":"/humanoid_robotics_textbook/perception-sensing","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"id":"perception-sensing","title":"3. Perception & Sensing","description":"Exploring the sensors and algorithms that allow a robot to perceive its environment.","sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Path Planning and Reinforcement Learning","permalink":"/humanoid_robotics_textbook/ai-robot-brain/path-planning-reinforcement-learning"},"next":{"title":"Voice Command Integration (OpenAI Whisper)","permalink":"/humanoid_robotics_textbook/vla/chapter-4.1-voice-command-integration"}}');var i=t(4848),a=t(8453),o=t(418),s=t(3457),l=t(1470),c=t(9365);const d={id:"perception-sensing",title:"3. Perception & Sensing",description:"Exploring the sensors and algorithms that allow a robot to perceive its environment.",sidebar_position:3},h=void 0,u={},m=[{value:"The Goal: From Raw Data to Understanding",id:"the-goal-from-raw-data-to-understanding",level:2},{value:"Exteroceptive Sensors: Seeing the World",id:"exteroceptive-sensors-seeing-the-world",level:3},{value:"Vision (Cameras)",id:"vision-cameras",level:4},{value:"LiDAR (Light Detection and Ranging)",id:"lidar-light-detection-and-ranging",level:4},{value:"Proprioceptive Sensors: Understanding Self",id:"proprioceptive-sensors-understanding-self",level:3},{value:"Inertial Measurement Unit (IMU)",id:"inertial-measurement-unit-imu",level:4},{value:"Joint Encoders",id:"joint-encoders",level:4},{value:"Force-Torque (F/T) Sensors",id:"force-torque-ft-sensors",level:4},{value:"The Power of Fusion",id:"the-power-of-fusion",level:3},{value:"Code Example: Basic Obstacle Detection",id:"code-example-basic-obstacle-detection",level:3}];function p(e){const n={em:"em",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"the-goal-from-raw-data-to-understanding",children:"The Goal: From Raw Data to Understanding"}),"\n",(0,i.jsxs)(n.p,{children:["A humanoid robot's ability to act meaningfully in the world is fundamentally limited by its ability to ",(0,i.jsx)(n.em,{children:"perceive"})," the world. Perception is not merely about collecting data; it's about transforming raw signals from a suite of sensors into a coherent, actionable understanding of the environment and the robot's own state within it."]}),"\n",(0,i.jsx)(n.p,{children:'In this chapter, we explore the primary sensors that give a robot its "senses" and the foundational algorithms that begin the process of turning data into knowledge.'}),"\n",(0,i.jsx)(o.A,{chart:'\ngraph TD\n  subgraph "Exteroceptive Sensors (Sensing the World)"\n      A[Camera] --\x3e|Image| D{Computer Vision};\n      B[LiDAR] --\x3e|Point Cloud| D;\n  end\n  subgraph "Proprioceptive Sensors (Sensing Self)"\n      E[IMU] --\x3e|Acceleration, Angular Velocity| F{State Estimation};\n      G[Joint Encoders] --\x3e|Joint Angles| F;\n      H[Force-Torque Sensors] --\x3e|Forces, Torques| F;\n  end\n  D --\x3e I(World Model);\n  F --\x3e I;\n\n  style A fill:#cce5ff;\n  style B fill:#cce5ff;\n  style E fill:#bde0fe;\n  style G fill:#bde0fe;\n  style H fill:#bde0fe;\n'}),"\n",(0,i.jsx)(n.h3,{id:"exteroceptive-sensors-seeing-the-world",children:"Exteroceptive Sensors: Seeing the World"}),"\n",(0,i.jsx)(n.p,{children:"These sensors gather information about the external environment."}),"\n",(0,i.jsx)(n.h4,{id:"vision-cameras",children:"Vision (Cameras)"}),"\n",(0,i.jsx)(n.p,{children:"Cameras are the richest source of data for a robot, providing dense information about color, texture, and shape. They are passive, low-power, and inexpensive."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monocular Cameras:"})," A single camera, like a human eye. It provides a 2D projection of the 3D world. Inferring depth is challenging but can be done with advanced AI models."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Stereo Cameras:"})," Two cameras spaced a known distance apart. By comparing the two images (finding a disparity map), the robot can calculate depth through triangulation, mimicking human stereoscopic vision."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RGB-D (Depth) Cameras:"})," These cameras directly provide a depth map alongside a standard color image. Common techniques include ",(0,i.jsx)(n.em,{children:"Time-of-Flight (ToF)"}),", which measures the time it takes for light to bounce back to the sensor."]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["The raw images from these cameras are processed by ",(0,i.jsx)(n.strong,{children:"computer vision algorithms"}),", which are now dominated by Deep Learning, specifically Convolutional Neural Networks (CNNs). Key tasks include:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Object Detection:"}),' Identifying and drawing bounding boxes around objects (e.g., "this is a cup").']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Semantic Segmentation:"})," Classifying every single pixel in an image (e.g., \"these pixels are 'road',\" \"these pixels are 'sky'\")."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pose Estimation:"})," Determining an object's precise 3D position and orientation relative to the camera."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"lidar-light-detection-and-ranging",children:"LiDAR (Light Detection and Ranging)"}),"\n",(0,i.jsx)(n.p,{children:"LiDAR sensors work by emitting rapid pulses of laser light and measuring the time it takes for the reflections to return."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Output:"})," This process generates a ",(0,i.jsx)(n.strong,{children:"point cloud"}),', which is a rich, 3D "map" of the surrounding environment.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Strengths:"})," LiDAR is highly accurate for measuring distance and is unaffected by lighting conditions, making it excellent for mapping, localization, and detecting obstacles."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Weaknesses:"})," It typically doesn't provide color information and can have trouble with reflective or transparent surfaces."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"proprioceptive-sensors-understanding-self",children:"Proprioceptive Sensors: Understanding Self"}),"\n",(0,i.jsx)(n.p,{children:"These sensors provide information about the robot's own state and movement."}),"\n",(0,i.jsx)(n.h4,{id:"inertial-measurement-unit-imu",children:"Inertial Measurement Unit (IMU)"}),"\n",(0,i.jsx)(n.p,{children:"The IMU is the core of the robot's sense of balance, located near its center of mass."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Components:"})," It contains an ",(0,i.jsx)(n.strong,{children:"accelerometer"})," (measures linear acceleration) and a ",(0,i.jsx)(n.strong,{children:"gyroscope"})," (measures angular velocity)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Function:"})," By integrating the data from the IMU, the robot can estimate its orientation (roll, pitch, yaw) and track its motion. It is absolutely critical for maintaining balance."]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"joint-encoders",children:"Joint Encoders"}),"\n",(0,i.jsx)(n.p,{children:"These are the most fundamental proprioceptive sensors. An encoder is attached to every joint (motor) on the robot and provides a precise measurement of the joint's angle. Without encoders, the robot would have no reliable knowledge of its own posture."}),"\n",(0,i.jsx)(n.h4,{id:"force-torque-ft-sensors",children:"Force-Torque (F/T) Sensors"}),"\n",(0,i.jsx)(n.p,{children:"Located in critical areas like the wrists and ankles, F/T sensors measure the forces and torques resulting from interaction with the environment. They allow the robot to feel when its foot is firmly on the ground or how much force it's exerting while grasping an object. This is essential for compliant motion and safe physical interaction."}),"\n",(0,i.jsx)(n.h3,{id:"the-power-of-fusion",children:"The Power of Fusion"}),"\n",(0,i.jsxs)(n.p,{children:["No single sensor is perfect. GPS can be inaccurate, IMUs drift over time, and camera-based estimates can be noisy. ",(0,i.jsx)(n.strong,{children:"Sensor fusion"})," is the process of intelligently combining data from multiple sensors to produce a state estimate that is more accurate, complete, and reliable than any individual sensor could provide."]}),"\n",(0,i.jsxs)(n.p,{children:["The most common tool for this is the ",(0,i.jsx)(n.strong,{children:"Kalman Filter"})," (and its variants like the Extended Kalman Filter for non-linear systems). It's a two-step process:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Predict:"})," The filter uses a motion model to predict the robot's state at the next point in time."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Update:"})," It then uses a new sensor measurement to correct and update this prediction."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"By fusing high-frequency data from an IMU with lower-frequency but more stable data from a camera or joint encoders, the robot can maintain a highly accurate and smooth estimate of its state."}),"\n",(0,i.jsx)(n.h3,{id:"code-example-basic-obstacle-detection",children:"Code Example: Basic Obstacle Detection"}),"\n",(0,i.jsx)(n.p,{children:"Let's simulate a basic perception task. Imagine we have a depth image from an RGB-D camera, represented as a 2D NumPy array where each value is the distance in meters. Our goal is to detect if there is an imminent obstacle directly in front of the robot."}),"\n",(0,i.jsx)(l.A,{children:(0,i.jsx)(c.A,{value:"python",label:"Python",children:(0,i.jsx)(s.A,{language:"python",children:'import numpy as np\n\ndef check_for_imminent_obstacle(depth_image, danger_zone_m, obstacle_threshold_m):\n  """\n  Checks a central region of a depth image for obstacles.\n  \n  Args:\n      depth_image (np.array): 2D array representing the depth image.\n      danger_zone_m (float): How close an object must be to be an obstacle.\n      obstacle_threshold_m (float): Percentage of pixels in the danger zone\n                                   that must be obstacles to trigger a warning.\n                                   \n  Returns:\n      bool: True if an obstacle is detected, False otherwise.\n  """\n  height, width = depth_image.shape\n  \n  # Define a central region of interest (e.g., middle 20% of the width)\n  roi_start = int(width // 2 - width * 0.1)\n  roi_end = int(width // 2 + width * 0.1)\n  center_region = depth_image[:, roi_start:roi_end]\n  \n  # Find all pixels where the distance is less than our danger zone\n  # and greater than zero (ignoring invalid depth readings)\n  obstacle_pixels = center_region[(center_region < danger_zone_m) & (center_region > 0)]\n  \n  # Calculate the percentage of the region that is considered an obstacle\n  obstacle_percentage = len(obstacle_pixels) / center_region.size\n  \n  print(f"Obstacle percentage in center region: {obstacle_percentage:.2%}")\n  \n  if obstacle_percentage > obstacle_threshold_m:\n      print("WARNING: Imminent obstacle detected!")\n      return True\n  else:\n      print("Path ahead is clear.")\n      return False\n\n# --- Simulation ---\n# Create a simulated 480x640 depth image, mostly clear (5 meters away)\nsimulated_depth = np.full((480, 640), 5.0)\n\n# Add a close obstacle (0.5 meters away) in the center\nsimulated_depth[180:300, 280:360] = 0.5\n\n# --- Run the check ---\n# An object is an obstacle if it\'s closer than 1.0 meter\n# and if more than 30% of the center region is blocked.\ncheck_for_imminent_obstacle(simulated_depth, danger_zone_m=1.0, obstacle_threshold_m=0.3)\n'})})})]})}function g(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(p,{...e})}):p(e)}}}]);